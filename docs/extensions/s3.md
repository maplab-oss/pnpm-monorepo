# S3

This guide covers setting up AWS S3 with LocalStack for local development, enabling a seamless transition to production.

## Environment Variables

This extension adds the following environment variables:

| Variable | Services | Secret | Notes |
|----------|----------|--------|-------|
| `AWS_REGION` | backend, worker | No | Defaults to `us-east-1` |
| `AWS_ACCESS_KEY_ID` | backend, worker | Yes | `test` locally, real key in prod |
| `AWS_SECRET_ACCESS_KEY` | backend, worker | Yes | `test` locally, real key in prod |
| `AWS_ENDPOINT_URL` | backend, worker | No | LocalStack URL locally, unset in prod |
| `AWS_S3_BUCKET` | backend, worker | No | Bucket name |

After adding this extension, update:
- [docs/env-vars.md](../env-vars.md) - Add to the registry table
- [deployment-runbook.md](../deployment-runbook.md) - Add AWS credentials to secrets table

## The Core Idea

LocalStack emulates AWS services locally. The key to seamless local-to-production parity is using a single environment variable—`AWS_ENDPOINT_URL`—to switch between LocalStack and real AWS:

- **Local**: Set `AWS_ENDPOINT_URL` to LocalStack's URL
- **Production**: Don't set `AWS_ENDPOINT_URL` (or leave it undefined)

The S3 client detects the endpoint and behaves accordingly.

## Environment Variables

| Variable | Local | Production |
|----------|-------|------------|
| `AWS_REGION` | `us-east-1` | Your actual region |
| `AWS_ACCESS_KEY_ID` | `test` | Real AWS key |
| `AWS_SECRET_ACCESS_KEY` | `test` | Real AWS secret |
| `AWS_ENDPOINT_URL` | `http://localhost:<port>` | _(not set)_ |
| `AWS_S3_BUCKET` | `helloworld-files` | Your bucket name |

**Critical insight**: When `AWS_ENDPOINT_URL` is not set, the AWS SDK automatically uses the real AWS endpoints. This means the same code works locally and in production with zero changes—just different environment variables.

### Environment Validation

```typescript
import { z } from "zod";

const envSchema = z.object({
  AWS_REGION: z.string().default("us-east-1"),
  AWS_ACCESS_KEY_ID: z.string().default("test"),
  AWS_SECRET_ACCESS_KEY: z.string().default("test"),
  AWS_ENDPOINT_URL: z.string().optional(),
  AWS_S3_BUCKET: z.string(),
  NODE_ENV: z.string().optional(),
});

export const env = envSchema.parse(process.env);
```

Using defaults for credentials means local development works out-of-the-box without extra configuration.

## LocalStack Configuration

Generate a random port for LocalStack:
```bash
etc/bin/randomport  # e.g., 4566
```

### Docker Compose / Service Definition

```yaml
localstack:
  image: localstack/localstack:latest
  ports:
    - "4566:4566"  # Replace 4566 with your generated port
  environment:
    - SERVICES=s3
    - DEBUG=0
    - DATA_DIR=/data
    - LS_LOG=warn
    - AWS_DEFAULT_REGION=us-east-1
  volumes:
    - localstack-data:/data
```

Key settings:
- **Port 4566**: LocalStack's gateway for all AWS services
- **SERVICES=s3**: Only run S3 (faster startup)
- **DATA_DIR=/data**: Persist data between restarts

## S3 Client Setup

```typescript
import { S3Client } from "@aws-sdk/client-s3";
import { env } from "./env";

let s3Client: S3Client | null = null;

export const getS3Client = (): S3Client => {
  if (s3Client) return s3Client;

  const config: ConstructorParameters<typeof S3Client>[0] = {
    region: env.AWS_REGION,
    credentials: {
      accessKeyId: env.AWS_ACCESS_KEY_ID,
      secretAccessKey: env.AWS_SECRET_ACCESS_KEY,
    },
  };

  if (env.AWS_ENDPOINT_URL) {
    config.endpoint = env.AWS_ENDPOINT_URL;
    config.forcePathStyle = true;
  }

  s3Client = new S3Client(config);
  return s3Client;
};
```

**Why `forcePathStyle: true`?** 

AWS uses virtual-hosted-style URLs by default (`bucket.s3.region.amazonaws.com`), but LocalStack requires path-style URLs (`localhost:4566/bucket`). This option is only needed when `AWS_ENDPOINT_URL` is set.

## Auto-Creating Buckets Locally

In development, you may want buckets created automatically:

```typescript
import {
  CreateBucketCommand,
  HeadBucketCommand,
} from "@aws-sdk/client-s3";

const ensureBucketExists = async (bucket: string): Promise<void> => {
  const isDev = env.AWS_ENDPOINT_URL || env.NODE_ENV !== "production";
  if (!isDev) return;

  const client = getS3Client();

  try {
    await client.send(new HeadBucketCommand({ Bucket: bucket }));
  } catch (error: unknown) {
    const err = error as {
      name?: string;
      $metadata?: { httpStatusCode?: number };
    };

    if (err.name === "NotFound" || err.$metadata?.httpStatusCode === 404) {
      await client.send(new CreateBucketCommand({ Bucket: bucket }));
    } else {
      throw error;
    }
  }
};
```

Call this before upload operations. In production, buckets should be pre-created via infrastructure-as-code.

## Basic Operations

### Upload

```typescript
import { PutObjectCommand } from "@aws-sdk/client-s3";

export const uploadToS3 = async (
  buffer: Buffer,
  key: string,
  contentType: string,
): Promise<void> => {
  const client = getS3Client();
  await ensureBucketExists(env.AWS_S3_BUCKET);

  await client.send(
    new PutObjectCommand({
      Bucket: env.AWS_S3_BUCKET,
      Key: key,
      Body: buffer,
      ContentType: contentType,
    }),
  );
};
```

### Download

```typescript
import { GetObjectCommand } from "@aws-sdk/client-s3";

export const downloadFromS3 = async (key: string): Promise<Buffer> => {
  const client = getS3Client();

  const response = await client.send(
    new GetObjectCommand({
      Bucket: env.AWS_S3_BUCKET,
      Key: key,
    }),
  );

  const stream = response.Body;
  const chunks: Uint8Array[] = [];
  for await (const chunk of stream as AsyncIterable<Uint8Array>) {
    chunks.push(chunk);
  }
  return Buffer.concat(chunks);
};
```

## Dependencies

```bash
pnpm add @aws-sdk/client-s3
```

## Checklist

### Local Development

1. Add LocalStack to your Docker/service configuration
2. Set environment variables:
   - `AWS_ENDPOINT_URL=http://localhost:<port>` (use your generated port)
   - `AWS_ACCESS_KEY_ID=test`
   - `AWS_SECRET_ACCESS_KEY=test`
3. Enable `forcePathStyle` when endpoint is set
4. Auto-create buckets in dev mode

### Production

1. Create S3 bucket via Terraform (see Infrastructure section below)
2. Configure environment variables:
   - `AWS_REGION` - Your region
   - `AWS_ACCESS_KEY_ID` - Real credentials
   - `AWS_SECRET_ACCESS_KEY` - Real credentials
   - `AWS_S3_BUCKET` - Your bucket name
   - **Do not set** `AWS_ENDPOINT_URL`

## Infrastructure (Terraform)

Create `infra/storage.tf`:

```hcl
resource "aws_s3_bucket" "files" {
  bucket = "${var.project_name}-files"
}

resource "aws_s3_bucket_public_access_block" "files" {
  bucket = aws_s3_bucket.files.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_iam_user" "s3_user" {
  name = "${var.project_name}-s3"
}

resource "aws_iam_access_key" "s3_user" {
  user = aws_iam_user.s3_user.name
}

resource "aws_iam_user_policy" "s3_user" {
  name = "${var.project_name}-s3-access"
  user = aws_iam_user.s3_user.name

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.files.arn,
          "${aws_s3_bucket.files.arn}/*"
        ]
      }
    ]
  })
}

output "s3_access_key_id" {
  value     = aws_iam_access_key.s3_user.id
  sensitive = true
}

output "s3_secret_access_key" {
  value     = aws_iam_access_key.s3_user.secret
  sensitive = true
}
```

Add AWS provider to `infra/main.tf`:

```hcl
provider "aws" {
  region = var.aws_region
}
```

Add to `infra/variables.tf`:

```hcl
variable "aws_region" {
  description = "AWS region for S3"
  type        = string
  default     = "us-east-1"
}
```

After `terraform apply`, get the credentials:

```bash
terraform output -raw s3_access_key_id
terraform output -raw s3_secret_access_key
```

Add these as GitHub Secrets (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`) and pass to the backend service

## Troubleshooting

### Connection Refused
LocalStack isn't running or port mapping is incorrect. Verify with:
```bash
curl http://localhost:4566/_localstack/health
```

### Invalid Bucket Name
LocalStack is stricter about bucket names than AWS in some versions. Use lowercase, no underscores.

### Path-Style vs Virtual-Hosted-Style Errors
Ensure `forcePathStyle: true` is set when using LocalStack. The AWS SDK defaults to virtual-hosted-style, which LocalStack doesn't support.
